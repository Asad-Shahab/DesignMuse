{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "90OhTzqbHcm3",
        "PWXnUIdL5BMX",
        "Nn-EIwMP5_om",
        "RdgF02-SG4Y2",
        "aH3K7YgY6E1f",
        "ykYBS0fKGwNl",
        "JjwCDs-MIAqZ"
      ],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOx1cDVrq5z83pVHucNWO8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asad-Shahab/DesignMuse/blob/main/Attention_Flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#____________________________\n"
      ],
      "metadata": {
        "id": "5iNaVM3cteuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch transformers plotly dash imageio dash_bootstrap_components bertviz sentencepiece"
      ],
      "metadata": {
        "id": "bBAcrFiNXxII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to display widgets in colab\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "vMLeam9CTocc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO-w0sEEcUAh",
        "outputId": "75e8cd06-d03b-4e92-9c44-d19f8ee29058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `attention flow` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `attention flow`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers.generation')\n",
        "\n",
        "def load_model(model_name=\"unsloth/Llama-3.2-1B-Instruct\"):\n",
        "    print(f\"Loading model: {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=dtype,\n",
        "        ).to(device)\n",
        "        print(f\"Model loaded on {device} with dtype {dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Attempting to load without specific dtype...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "        print(f\"Model loaded on {device} (default dtype)\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token:\n",
        "            print(\"Setting pad_token to eos_token\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            if hasattr(model.config, 'pad_token_id') and model.config.pad_token_id is None:\n",
        "                 model.config.pad_token_id = tokenizer.eos_token_id\n",
        "        else:\n",
        "            print(\"Warning: No eos_token found to set as pad_token.\")\n",
        "\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def generate_with_attention(model, tokenizer, prompt, device, max_tokens=30):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    input_len_raw = input_ids.shape[1]\n",
        "\n",
        "    print(f\"Generating with input length: {input_len_raw}, max_new_tokens: {max_tokens}\")\n",
        "    with torch.no_grad():\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        gen_kwargs = {\n",
        "             \"attention_mask\": attention_mask,\n",
        "             \"max_new_tokens\": max_tokens,\n",
        "             \"output_attentions\": True,\n",
        "             \"return_dict_in_generate\": True,\n",
        "        }\n",
        "        if tokenizer.pad_token_id is not None:\n",
        "             gen_kwargs[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "\n",
        "        try:\n",
        "            output = model.generate(input_ids, **gen_kwargs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            return None, [], [], 0, \"Error during generation.\"\n",
        "\n",
        "    full_sequence = output.sequences[0]\n",
        "    if full_sequence.shape[0] > input_len_raw:\n",
        "         generated_ids = full_sequence[input_len_raw:]\n",
        "    else:\n",
        "         generated_ids = torch.tensor([], dtype=torch.long, device=device)\n",
        "\n",
        "    output_tokens = tokenizer.convert_ids_to_tokens(generated_ids, skip_special_tokens=False)\n",
        "    input_tokens_raw = tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=False)\n",
        "\n",
        "    input_tokens = input_tokens_raw\n",
        "    input_len_for_attention = input_len_raw\n",
        "    bos_token = tokenizer.bos_token or '<|begin_of_text|>'\n",
        "\n",
        "    if input_tokens_raw and input_tokens_raw[0] == bos_token:\n",
        "         input_tokens = input_tokens_raw[1:]\n",
        "         input_len_for_attention = input_len_raw - 1\n",
        "\n",
        "    eos_token = tokenizer.eos_token or '<|end_of_text|>'\n",
        "    if output_tokens and output_tokens[-1] == eos_token:\n",
        "        output_tokens = output_tokens[:-1]\n",
        "        generated_ids = generated_ids[:-1]\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    attentions = getattr(output, 'attentions', None)\n",
        "    if attentions is None:\n",
        "        print(\"Warning: 'attentions' not found in model output. Cannot visualize attention.\")\n",
        "        return None, output_tokens, input_tokens, input_len_for_attention, generated_text\n",
        "\n",
        "    return attentions, output_tokens, input_tokens, input_len_for_attention, generated_text\n",
        "\n",
        "def clean_label(token):\n",
        "    \"\"\"Cleans token labels for visualization.\"\"\"\n",
        "    label = str(token)\n",
        "    label = label.replace('Ġ', ' ')\n",
        "    label = label.replace(' ', ' ')\n",
        "    label = label.replace('Ċ', '\\\\n')\n",
        "    label = label.replace('</s>', '[EOS]')\n",
        "    label = label.replace('<unk>', '[UNK]')\n",
        "    label = label.replace('<|begin_of_text|>', '[BOS]')\n",
        "    label = label.replace('<|end_of_text|>', '[EOS]')\n",
        "    label = re.sub(r'<0x[0-9A-Fa-f]{2}>', '', label)\n",
        "    label = label.strip()\n",
        "    return label if label else \"[EMPTY]\"\n",
        "\n",
        "# --- Attention Processing Functions ---\n",
        "\n",
        "# Function to process attention with SEPARATE normalization\n",
        "def process_attention_flow_separate(attentions, input_len_for_attention, output_len):\n",
        "    if not attentions:\n",
        "        return [{'input_attention': torch.zeros(input_len_for_attention),\n",
        "                 'output_attention': None} for _ in range(output_len)]\n",
        "\n",
        "    attention_matrices = []\n",
        "    num_steps = len(attentions)\n",
        "    if num_steps == 0:\n",
        "        print(\"Warning: No attention steps found in output (Separate Norm).\")\n",
        "        return [{'input_attention': torch.zeros(input_len_for_attention),\n",
        "                 'output_attention': None} for _ in range(output_len)]\n",
        "\n",
        "    steps_to_process = min(num_steps, output_len)\n",
        "\n",
        "    for i in range(steps_to_process):\n",
        "        step_attentions = attentions[i]\n",
        "        input_attention_layers = []\n",
        "        output_attention_layers = []\n",
        "\n",
        "        for layer_idx, layer_attn in enumerate(step_attentions):\n",
        "            try:\n",
        "                input_indices = slice(1, 1 + input_len_for_attention)\n",
        "                if layer_attn.shape[3] >= input_indices.stop:\n",
        "                    input_attn = layer_attn[0, :, 0, input_indices]\n",
        "                    input_attention_layers.append(input_attn)\n",
        "                    if i > 0:\n",
        "                        output_indices = slice(1 + input_len_for_attention, 1 + input_len_for_attention + i)\n",
        "                        if layer_attn.shape[3] >= output_indices.stop:\n",
        "                            output_attn = layer_attn[0, :, 0, output_indices]\n",
        "                            output_attention_layers.append(output_attn)\n",
        "                        else:\n",
        "                            output_attention_layers.append(torch.zeros((layer_attn.shape[1], i), device=layer_attn.device))\n",
        "                else:\n",
        "                    input_attention_layers.append(torch.zeros((layer_attn.shape[1], input_len_for_attention), device=layer_attn.device))\n",
        "                    if i > 0:\n",
        "                        output_attention_layers.append(torch.zeros((layer_attn.shape[1], i), device=layer_attn.device))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing separate attention at step {i}, layer {layer_idx}: {e}\")\n",
        "                input_attention_layers.append(torch.zeros((layer_attn.shape[1], input_len_for_attention), device=layer_attn.device))\n",
        "                if i > 0:\n",
        "                    output_attention_layers.append(torch.zeros((layer_attn.shape[1], i), device=layer_attn.device))\n",
        "\n",
        "        if input_attention_layers:\n",
        "            avg_input_attn = torch.mean(torch.stack(input_attention_layers).float(), dim=[0, 1])\n",
        "        else:\n",
        "            avg_input_attn = torch.zeros(input_len_for_attention)\n",
        "\n",
        "        avg_output_attn = None\n",
        "        if i > 0 and output_attention_layers:\n",
        "            avg_output_attn = torch.mean(torch.stack(output_attention_layers).float(), dim=[0, 1])\n",
        "        elif i > 0:\n",
        "            avg_output_attn = torch.zeros(i)\n",
        "\n",
        "        input_sum = avg_input_attn.sum() + 1e-8\n",
        "        normalized_input_attn = avg_input_attn / input_sum\n",
        "\n",
        "        normalized_output_attn = None\n",
        "        if i > 0 and avg_output_attn is not None:\n",
        "            output_sum = avg_output_attn.sum() + 1e-8\n",
        "            normalized_output_attn = avg_output_attn / output_sum\n",
        "\n",
        "        attention_matrices.append({\n",
        "            'input_attention': normalized_input_attn.cpu(),\n",
        "            'output_attention': normalized_output_attn.cpu() if normalized_output_attn is not None else None,\n",
        "            'raw_input_attention': avg_input_attn.cpu(), # Keep raw for analysis\n",
        "            'raw_output_attention': avg_output_attn.cpu() if avg_output_attn is not None else None\n",
        "        })\n",
        "\n",
        "    while len(attention_matrices) < output_len:\n",
        "        attention_matrices.append({\n",
        "            'input_attention': torch.zeros(input_len_for_attention),\n",
        "            'output_attention': None,\n",
        "            'raw_input_attention': torch.zeros(input_len_for_attention),\n",
        "            'raw_output_attention': None\n",
        "        })\n",
        "    return attention_matrices\n",
        "\n",
        "# Function to process attention with JOINT normalization\n",
        "def process_attention_flow_joint(attentions, input_len_for_attention, output_len):\n",
        "    if not attentions:\n",
        "        return [{'input_attention': torch.zeros(input_len_for_attention),\n",
        "                 'output_attention': None} for _ in range(output_len)]\n",
        "\n",
        "    attention_matrices = []\n",
        "    num_steps = len(attentions)\n",
        "    if num_steps == 0:\n",
        "        print(\"Warning: No attention steps found in output (Joint Norm).\")\n",
        "        return [{'input_attention': torch.zeros(input_len_for_attention),\n",
        "                 'output_attention': None} for _ in range(output_len)]\n",
        "\n",
        "    steps_to_process = min(num_steps, output_len)\n",
        "\n",
        "    for i in range(steps_to_process):\n",
        "        step_attentions = attentions[i]\n",
        "        input_attention_layers = []\n",
        "        output_attention_layers = []\n",
        "\n",
        "        for layer_idx, layer_attn in enumerate(step_attentions):\n",
        "            try:\n",
        "                input_indices = slice(1, 1 + input_len_for_attention)\n",
        "                if layer_attn.shape[3] >= input_indices.stop:\n",
        "                    input_attn = layer_attn[0, :, 0, input_indices]\n",
        "                    input_attention_layers.append(input_attn)\n",
        "                    if i > 0:\n",
        "                        output_indices = slice(1 + input_len_for_attention, 1 + input_len_for_attention + i)\n",
        "                        if layer_attn.shape[3] >= output_indices.stop:\n",
        "                            output_attn = layer_attn[0, :, 0, output_indices]\n",
        "                            output_attention_layers.append(output_attn)\n",
        "                        else:\n",
        "                            output_attention_layers.append(torch.zeros((layer_attn.shape[1], i), device=layer_attn.device))\n",
        "                else:\n",
        "                    input_attention_layers.append(torch.zeros((layer_attn.shape[1], input_len_for_attention), device=layer_attn.device))\n",
        "                    if i > 0:\n",
        "                        output_attention_layers.append(torch.zeros((layer_attn.shape[1], i), device=layer_attn.device))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing joint attention at step {i}, layer {layer_idx}: {e}\")\n",
        "                input_attention_layers.append(torch.zeros((layer_attn.shape[1], input_len_for_attention), device=layer_attn.device))\n",
        "                if i > 0:\n",
        "                    output_attention_layers.append(torch.zeros((layer_attn.shape[1], i), device=layer_attn.device))\n",
        "\n",
        "        if input_attention_layers:\n",
        "            avg_input_attn = torch.mean(torch.stack(input_attention_layers).float(), dim=[0, 1])\n",
        "        else:\n",
        "            avg_input_attn = torch.zeros(input_len_for_attention)\n",
        "\n",
        "        avg_output_attn = None\n",
        "        if i > 0 and output_attention_layers:\n",
        "            avg_output_attn = torch.mean(torch.stack(output_attention_layers).float(), dim=[0, 1])\n",
        "        elif i > 0:\n",
        "            avg_output_attn = torch.zeros(i)\n",
        "\n",
        "        if i > 0 and avg_output_attn is not None:\n",
        "            combined_attn = torch.cat([avg_input_attn, avg_output_attn])\n",
        "            sum_attn = combined_attn.sum() + 1e-8\n",
        "            normalized_combined = combined_attn / sum_attn\n",
        "            normalized_input_attn = normalized_combined[:input_len_for_attention]\n",
        "            normalized_output_attn = normalized_combined[input_len_for_attention:]\n",
        "        else:\n",
        "            sum_attn = avg_input_attn.sum() + 1e-8\n",
        "            normalized_input_attn = avg_input_attn / sum_attn\n",
        "            normalized_output_attn = None\n",
        "\n",
        "        attention_matrices.append({\n",
        "            'input_attention': normalized_input_attn.cpu(),\n",
        "            'output_attention': normalized_output_attn.cpu() if normalized_output_attn is not None else None\n",
        "        })\n",
        "\n",
        "    while len(attention_matrices) < output_len:\n",
        "        attention_matrices.append({\n",
        "            'input_attention': torch.zeros(input_len_for_attention),\n",
        "            'output_attention': None\n",
        "        })\n",
        "\n",
        "    return attention_matrices\n",
        "\n",
        "\n",
        "# --- Visualization Creation Functions ---\n",
        "\n",
        "def create_flow_visualization_separate(input_tokens, output_tokens, attention_matrices, strong_threshold=0.05):\n",
        "    title = \"Complete LLM Attention Flow Visualization (Separate Normalization)\"\n",
        "    normalization_label = \"Separately Normalized\"\n",
        "\n",
        "    input_labels = [clean_label(token) for token in input_tokens]\n",
        "    output_labels = [clean_label(token) for token in output_tokens]\n",
        "    num_input = len(input_labels)\n",
        "    num_output = len(output_labels)\n",
        "    num_steps = len(attention_matrices)\n",
        "\n",
        "    if num_input == 0 or num_output == 0 or num_steps == 0:\n",
        "        print(\"Warning: No tokens or attention matrices to visualize (Separate Norm).\")\n",
        "        fig = go.Figure()\n",
        "        fig.update_layout(title=f\"{title} (No Data)\", xaxis={'visible': False}, yaxis={'visible': False})\n",
        "        figw = go.FigureWidget(fig)\n",
        "        def dummy_update(step): pass\n",
        "        return figw, dummy_update\n",
        "\n",
        "    input_y = np.linspace(0.1, 0.9, num_input) if num_input > 1 else np.array([0.5])\n",
        "    output_y = np.linspace(0.1, 0.9, num_output) if num_output > 1 else np.array([0.5])\n",
        "    input_x = np.full(num_input, 0.1)\n",
        "    output_x = np.full(num_output, 0.9)\n",
        "    min_line_width, max_line_width = 0.5, 6.0\n",
        "\n",
        "    input_to_output_traces, input_to_output_info = [], []\n",
        "    for j in range(num_output):\n",
        "        for i in range(num_input):\n",
        "            conn_trace = go.Scatter(x=[input_x[i], output_x[j]], y=[input_y[i], output_y[j]], mode=\"lines\", line=dict(color=\"rgba(0, 0, 255, 0.6)\", width=min_line_width), opacity=0, showlegend=False, visible=True, hoverinfo='text', text=f\"<b>{input_labels[i]} → {output_labels[j]}</b><br>Weight: 0.0000\", hoverlabel=dict(bgcolor=\"lightskyblue\", bordercolor=\"darkblue\"), customdata=[(i, j)], name=f\"sep_in_to_out_{i}_{j}\")\n",
        "            input_to_output_traces.append(conn_trace)\n",
        "            input_to_output_info.append({'input_idx': i, 'output_idx': j, 'trace_idx': len(input_to_output_traces) - 1})\n",
        "\n",
        "    output_to_output_traces, output_to_output_info = [], []\n",
        "    control_x_offset = 0.15\n",
        "    for j in range(1, num_output):\n",
        "        for i in range(j):\n",
        "            path_x = [output_x[i], output_x[i] + control_x_offset, output_x[j] + control_x_offset, output_x[j]]\n",
        "            path_y = [output_y[i], output_y[i], output_y[j], output_y[j]]\n",
        "            conn_trace = go.Scatter(x=path_x, y=path_y, mode=\"lines\", line=dict(color=\"rgba(255, 0, 0, 0.6)\", width=min_line_width, shape='spline'), opacity=0, showlegend=False, visible=True, hoverinfo='text', text=f\"<b>{output_labels[i]} → {output_labels[j]}</b><br>Weight: 0.0000\", hoverlabel=dict(bgcolor=\"lightcoral\", bordercolor=\"darkred\"), customdata=[(i, j)], name=f\"sep_out_to_out_{i}_{j}\")\n",
        "            output_to_output_traces.append(conn_trace)\n",
        "            output_to_output_info.append({'from_idx': i, 'to_idx': j, 'trace_idx': len(input_to_output_traces) + len(output_to_output_traces) - 1})\n",
        "\n",
        "    input_trace = go.Scatter(x=input_x, y=input_y, mode=\"markers+text\", marker=dict(size=15, color=\"skyblue\", line=dict(width=1, color=\"darkblue\")), text=input_labels, textfont=dict(size=10), textposition=\"middle left\", name=\"Input Tokens\", hovertemplate=\"<b>Input:</b> %{text}<extra></extra>\")\n",
        "    output_trace = go.Scatter(x=output_x, y=output_y, mode=\"markers+text\", marker=dict(size=15, color=[\"rgba(230, 230, 230, 0.8)\"] * num_output, line=dict(width=1, color=\"darkred\")), text=output_labels, textfont=dict(size=10), textposition=\"middle right\", name=\"Output Tokens\", hovertemplate=\"<b>Output:</b> %{text}<extra></extra>\")\n",
        "\n",
        "    data = input_to_output_traces + output_to_output_traces + [input_trace, output_trace]\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis=dict(range=[-0.1, 1.1], showgrid=False, zeroline=False, showticklabels=False, fixedrange=True),\n",
        "        yaxis=dict(range=[0, 1], showgrid=False, zeroline=False, showticklabels=False, fixedrange=True),\n",
        "        hovermode=\"closest\", width=1000, height=max(600, num_input * 30, num_output * 20),\n",
        "        plot_bgcolor=\"white\", margin=dict(l=120, r=120, t=50, b=50), hoverdistance=10,\n",
        "        hoverlabel=dict(font_size=12, font_family=\"Arial\")\n",
        "    )\n",
        "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='rgba(0, 0, 255, 0.6)', width=2), name='Input→Output Attn'))\n",
        "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='rgba(255, 0, 0, 0.6)', width=2, shape='spline'), name='Output→Output Attn'))\n",
        "\n",
        "    step_annotation = fig.add_annotation(x=0.5, y=0.02, text=f\"Step 0 / {num_steps-1}: {clean_label(output_tokens[0]) if output_tokens else ''}\", showarrow=False, font=dict(size=12, color=\"darkred\"), xref=\"paper\", yref=\"paper\")\n",
        "    tooltip_annotation = fig.add_annotation(x=0.01, y=0.98, text=\"Hover connections for attention weight\", showarrow=False, font=dict(size=10, color=\"gray\"), align=\"left\", xref=\"paper\", yref=\"paper\")\n",
        "    text_annotation = fig.add_annotation(x=0.5, y=-0.05, text=\"Generated: \", showarrow=False, font=dict(size=12), xref=\"paper\", yref=\"paper\")\n",
        "\n",
        "    figw = go.FigureWidget(fig)\n",
        "    input_node_trace_idx = len(input_to_output_traces) + len(output_to_output_traces)\n",
        "    output_node_trace_idx = input_node_trace_idx + 1\n",
        "\n",
        "    def update_visualization_separate_internal(step):\n",
        "        if step == -1: # All Connections\n",
        "            with figw.batch_update():\n",
        "                figw.layout.annotations[0].text = \"All Connections Above Threshold\"\n",
        "                generated_so_far = \" \".join([clean_label(token) for token in output_tokens])\n",
        "                figw.layout.annotations[2].text = f\"Generated: {generated_so_far}\"\n",
        "\n",
        "                # Reset all opacities first\n",
        "                for k in range(len(input_to_output_traces) + len(output_to_output_traces)):\n",
        "                    figw.data[k].opacity = 0\n",
        "\n",
        "                # Iterate through steps and apply thresholds\n",
        "                for j in range(num_output):\n",
        "                    current_attention = attention_matrices[j]\n",
        "                    # Input-to-Output\n",
        "                    if 'input_attention' in current_attention and current_attention['input_attention'] is not None:\n",
        "                        input_attn = current_attention['input_attention']\n",
        "                        max_input_weight = input_attn.max().item() if input_attn.numel() > 0 else 1.0\n",
        "                        if max_input_weight <= 1e-8: max_input_weight = 1.0\n",
        "                        for conn in input_to_output_info:\n",
        "                            i, out_j, trace_idx = conn['input_idx'], conn['output_idx'], conn['trace_idx']\n",
        "                            if out_j == j and i < len(input_attn):\n",
        "                                weight = input_attn[i].item()\n",
        "                                if weight >= strong_threshold:\n",
        "                                    scaled_weight = weight / max_input_weight\n",
        "                                    width = min_line_width + (max_line_width - min_line_width) * scaled_weight\n",
        "                                    opacity = min(1.0, scaled_weight * 5)\n",
        "                                    figw.data[trace_idx].line.width = width\n",
        "                                    figw.data[trace_idx].opacity = opacity\n",
        "                                    figw.data[trace_idx].text = f\"<b>{input_labels[i]} → {output_labels[j]}</b><br>Weight: {weight:.6f}<br>Step {j}<br>{normalization_label}\"\n",
        "\n",
        "                    # Output-to-Output\n",
        "                    if j > 0 and 'output_attention' in current_attention and current_attention['output_attention'] is not None:\n",
        "                        output_attn = current_attention['output_attention']\n",
        "                        max_output_weight = output_attn.max().item() if output_attn.numel() > 0 else 1.0\n",
        "                        if max_output_weight <= 1e-8: max_output_weight = 1.0\n",
        "                        for conn in output_to_output_info:\n",
        "                            from_idx, to_idx, trace_idx = conn['from_idx'], conn['to_idx'], conn['trace_idx']\n",
        "                            if to_idx == j and from_idx < len(output_attn):\n",
        "                                weight = output_attn[from_idx].item()\n",
        "                                if weight >= strong_threshold:\n",
        "                                    scaled_weight = weight / max_output_weight\n",
        "                                    width = min_line_width + (max_line_width - min_line_width) * scaled_weight\n",
        "                                    opacity = min(1.0, scaled_weight * 5)\n",
        "                                    figw.data[trace_idx].line.width = width\n",
        "                                    figw.data[trace_idx].opacity = opacity\n",
        "                                    figw.data[trace_idx].text = f\"<b>{output_labels[from_idx]} → {output_labels[to_idx]}</b><br>Weight: {weight:.6f}<br>Step {j}<br>{normalization_label}\"\n",
        "\n",
        "                figw.data[output_node_trace_idx].marker.color = [\"lightcoral\"] * num_output\n",
        "            return\n",
        "\n",
        "        if step >= num_steps or step < 0: return\n",
        "\n",
        "        with figw.batch_update():\n",
        "            figw.layout.annotations[0].text = f\"Step {step} / {num_steps-1}: {clean_label(output_tokens[step])}\"\n",
        "            generated_so_far = \" \".join([clean_label(token) for token in output_tokens[:step+1]])\n",
        "            figw.layout.annotations[2].text = f\"Generated: {generated_so_far}\"\n",
        "\n",
        "            current_attention = attention_matrices[step]\n",
        "\n",
        "            # Update Input-to-Output Connections\n",
        "            if 'input_attention' in current_attention and current_attention['input_attention'] is not None:\n",
        "                input_attn = current_attention['input_attention']\n",
        "                max_input_weight = input_attn.max().item() if input_attn.numel() > 0 else 1.0\n",
        "                if max_input_weight <= 1e-8: max_input_weight = 1.0\n",
        "                for conn in input_to_output_info:\n",
        "                    trace_idx, i, j = conn['trace_idx'], conn['input_idx'], conn['output_idx']\n",
        "                    if j == step and i < len(input_attn):\n",
        "                        weight = input_attn[i].item()\n",
        "                        scaled_weight = weight / max_input_weight\n",
        "                        width = min_line_width + (max_line_width - min_line_width) * scaled_weight\n",
        "                        opacity = min(1.0, scaled_weight * 5)\n",
        "                        is_visible = weight >= strong_threshold\n",
        "                        figw.data[trace_idx].line.width = width\n",
        "                        figw.data[trace_idx].opacity = opacity if is_visible else 0\n",
        "                        figw.data[trace_idx].text = f\"<b>{input_labels[i]} → {output_labels[j]}</b><br>Weight: {weight:.6f}<br>{normalization_label}\"\n",
        "                    else: figw.data[trace_idx].opacity = 0\n",
        "            else:\n",
        "                 for conn in input_to_output_info: figw.data[conn['trace_idx']].opacity = 0\n",
        "\n",
        "\n",
        "            # Update Output-to-Output Connections\n",
        "            if step > 0 and 'output_attention' in current_attention and current_attention['output_attention'] is not None:\n",
        "                output_attn = current_attention['output_attention']\n",
        "                max_output_weight = output_attn.max().item() if output_attn.numel() > 0 else 1.0\n",
        "                if max_output_weight <= 1e-8: max_output_weight = 1.0\n",
        "                for conn in output_to_output_info:\n",
        "                    trace_idx, from_idx, to_idx = conn['trace_idx'], conn['from_idx'], conn['to_idx']\n",
        "                    if to_idx == step and from_idx < len(output_attn):\n",
        "                        weight = output_attn[from_idx].item()\n",
        "                        scaled_weight = weight / max_output_weight\n",
        "                        width = min_line_width + (max_line_width - min_line_width) * scaled_weight\n",
        "                        opacity = min(1.0, scaled_weight * 5)\n",
        "                        is_visible = weight >= strong_threshold\n",
        "                        figw.data[trace_idx].line.width = width\n",
        "                        figw.data[trace_idx].opacity = opacity if is_visible else 0\n",
        "                        figw.data[trace_idx].text = f\"<b>{output_labels[from_idx]} → {output_labels[to_idx]}</b><br>Weight: {weight:.6f}<br>{normalization_label}\"\n",
        "                    else: figw.data[trace_idx].opacity = 0\n",
        "            else:\n",
        "                 for conn in output_to_output_info: figw.data[conn['trace_idx']].opacity = 0\n",
        "\n",
        "            # Update Output Node Highlighting\n",
        "            new_colors = [\"lightcoral\" if j == step else (\"lightpink\" if j < step else \"rgba(230, 230, 230, 0.8)\") for j in range(num_output)]\n",
        "            figw.data[output_node_trace_idx].marker.color = new_colors\n",
        "\n",
        "    return figw, update_visualization_separate_internal\n",
        "\n",
        "\n",
        "def create_flow_visualization_joint(input_tokens, output_tokens, attention_matrices, strong_threshold=0.01):\n",
        "    title = \"Complete LLM Attention Flow Visualization (Joint Normalization)\"\n",
        "    normalization_label = \"Jointly Normalized\"\n",
        "\n",
        "    input_labels = [clean_label(token) for token in input_tokens]\n",
        "    output_labels = [clean_label(token) for token in output_tokens]\n",
        "    num_input = len(input_labels)\n",
        "    num_output = len(output_labels)\n",
        "    num_steps = len(attention_matrices)\n",
        "\n",
        "    if num_input == 0 or num_output == 0 or num_steps == 0:\n",
        "        print(\"Warning: No tokens or attention matrices to visualize (Joint Norm).\")\n",
        "        fig = go.Figure()\n",
        "        fig.update_layout(title=f\"{title} (No Data)\", xaxis={'visible': False}, yaxis={'visible': False})\n",
        "        figw = go.FigureWidget(fig)\n",
        "        def dummy_update(step): pass\n",
        "        return figw, dummy_update\n",
        "\n",
        "    input_y = np.linspace(0.1, 0.9, num_input) if num_input > 1 else np.array([0.5])\n",
        "    output_y = np.linspace(0.1, 0.9, num_output) if num_output > 1 else np.array([0.5])\n",
        "    input_x = np.full(num_input, 0.1)\n",
        "    output_x = np.full(num_output, 0.9)\n",
        "    min_line_width, max_line_width = 0.5, 6.0\n",
        "\n",
        "    input_to_output_traces, input_to_output_info = [], []\n",
        "    for j in range(num_output):\n",
        "        for i in range(num_input):\n",
        "            conn_trace = go.Scatter(x=[input_x[i], output_x[j]], y=[input_y[i], output_y[j]], mode=\"lines\", line=dict(color=\"rgba(0, 0, 255, 0.6)\", width=min_line_width), opacity=0, showlegend=False, visible=True, hoverinfo='text', text=f\"<b>{input_labels[i]} → {output_labels[j]}</b><br>Weight: 0.0000\", hoverlabel=dict(bgcolor=\"lightskyblue\", bordercolor=\"darkblue\"), customdata=[(i, j)], name=f\"joint_in_to_out_{i}_{j}\")\n",
        "            input_to_output_traces.append(conn_trace)\n",
        "            input_to_output_info.append({'input_idx': i, 'output_idx': j, 'trace_idx': len(input_to_output_traces) - 1})\n",
        "\n",
        "    output_to_output_traces, output_to_output_info = [], []\n",
        "    control_x_offset = 0.15\n",
        "    for j in range(1, num_output):\n",
        "        for i in range(j):\n",
        "            path_x = [output_x[i], output_x[i] + control_x_offset, output_x[j] + control_x_offset, output_x[j]]\n",
        "            path_y = [output_y[i], output_y[i], output_y[j], output_y[j]]\n",
        "            conn_trace = go.Scatter(x=path_x, y=path_y, mode=\"lines\", line=dict(color=\"rgba(255, 0, 0, 0.6)\", width=min_line_width, shape='spline'), opacity=0, showlegend=False, visible=True, hoverinfo='text', text=f\"<b>{output_labels[i]} → {output_labels[j]}</b><br>Weight: 0.0000\", hoverlabel=dict(bgcolor=\"lightcoral\", bordercolor=\"darkred\"), customdata=[(i, j)], name=f\"joint_out_to_out_{i}_{j}\")\n",
        "            output_to_output_traces.append(conn_trace)\n",
        "            output_to_output_info.append({'from_idx': i, 'to_idx': j, 'trace_idx': len(input_to_output_traces) + len(output_to_output_traces) - 1})\n",
        "\n",
        "    input_trace = go.Scatter(x=input_x, y=input_y, mode=\"markers+text\", marker=dict(size=15, color=\"skyblue\", line=dict(width=1, color=\"darkblue\")), text=input_labels, textfont=dict(size=10), textposition=\"middle left\", name=\"Input Tokens\", hovertemplate=\"<b>Input:</b> %{text}<extra></extra>\")\n",
        "    output_trace = go.Scatter(x=output_x, y=output_y, mode=\"markers+text\", marker=dict(size=15, color=[\"rgba(230, 230, 230, 0.8)\"] * num_output, line=dict(width=1, color=\"darkred\")), text=output_labels, textfont=dict(size=10), textposition=\"middle right\", name=\"Output Tokens\", hovertemplate=\"<b>Output:</b> %{text}<extra></extra>\")\n",
        "\n",
        "    data = input_to_output_traces + output_to_output_traces + [input_trace, output_trace]\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis=dict(range=[-0.1, 1.1], showgrid=False, zeroline=False, showticklabels=False, fixedrange=True),\n",
        "        yaxis=dict(range=[0, 1], showgrid=False, zeroline=False, showticklabels=False, fixedrange=True),\n",
        "        hovermode=\"closest\", width=1000, height=max(600, num_input * 30, num_output * 20),\n",
        "        plot_bgcolor=\"white\", margin=dict(l=120, r=120, t=50, b=50), hoverdistance=10,\n",
        "        hoverlabel=dict(font_size=12, font_family=\"Arial\")\n",
        "    )\n",
        "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='rgba(0, 0, 255, 0.6)', width=2), name='Input→Output Attn'))\n",
        "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='rgba(255, 0, 0, 0.6)', width=2, shape='spline'), name='Output→Output Attn'))\n",
        "\n",
        "    step_annotation = fig.add_annotation(x=0.5, y=0.02, text=f\"Step 0 / {num_steps-1}: {clean_label(output_tokens[0]) if output_tokens else ''}\", showarrow=False, font=dict(size=12, color=\"darkred\"), xref=\"paper\", yref=\"paper\")\n",
        "    tooltip_annotation = fig.add_annotation(x=0.01, y=0.98, text=\"Hover connections for attention weight\", showarrow=False, font=dict(size=10, color=\"gray\"), align=\"left\", xref=\"paper\", yref=\"paper\")\n",
        "    text_annotation = fig.add_annotation(x=0.5, y=-0.05, text=\"Generated: \", showarrow=False, font=dict(size=12), xref=\"paper\", yref=\"paper\")\n",
        "\n",
        "    figw = go.FigureWidget(fig)\n",
        "    input_node_trace_idx = len(input_to_output_traces) + len(output_to_output_traces)\n",
        "    output_node_trace_idx = input_node_trace_idx + 1\n",
        "\n",
        "    def update_visualization_joint_internal(step):\n",
        "      if step == -1: # All Connections\n",
        "          with figw.batch_update():\n",
        "              figw.layout.annotations[0].text = \"All Connections Above Threshold\"\n",
        "              generated_so_far = \" \".join([clean_label(token) for token in output_tokens])\n",
        "              figw.layout.annotations[2].text = f\"Generated: {generated_so_far}\"\n",
        "\n",
        "              # Reset all opacities first\n",
        "              for k in range(len(input_to_output_traces) + len(output_to_output_traces)):\n",
        "                  figw.data[k].opacity = 0\n",
        "\n",
        "              # Iterate through steps and apply thresholds\n",
        "              for j in range(num_output):\n",
        "                  current_attention = attention_matrices[j]\n",
        "                  # Input-to-Output\n",
        "                  if 'input_attention' in current_attention and current_attention['input_attention'] is not None:\n",
        "                      input_attn = current_attention['input_attention']\n",
        "                      for conn in input_to_output_info:\n",
        "                          i, out_j, trace_idx = conn['input_idx'], conn['output_idx'], conn['trace_idx']\n",
        "                          if out_j == j and i < len(input_attn):\n",
        "                              weight = input_attn[i].item()\n",
        "                              if weight >= strong_threshold:\n",
        "                                  width = min_line_width + (max_line_width - min_line_width) * weight * 5 # Scale width by weight\n",
        "                                  opacity = min(1.0, weight * 10) # Scale opacity more aggressively for joint\n",
        "                                  figw.data[trace_idx].line.width = width\n",
        "                                  figw.data[trace_idx].opacity = opacity\n",
        "                                  figw.data[trace_idx].text = f\"<b>{input_labels[i]} → {output_labels[j]}</b><br>Weight: {weight:.6f}<br>Step {j}<br>{normalization_label}\"\n",
        "\n",
        "                  # Output-to-Output\n",
        "                  if j > 0 and 'output_attention' in current_attention and current_attention['output_attention'] is not None:\n",
        "                      output_attn = current_attention['output_attention']\n",
        "                      for conn in output_to_output_info:\n",
        "                          from_idx, to_idx, trace_idx = conn['from_idx'], conn['to_idx'], conn['trace_idx']\n",
        "                          if to_idx == j and from_idx < len(output_attn):\n",
        "                              weight = output_attn[from_idx].item()\n",
        "                              if weight >= strong_threshold:\n",
        "                                  width = min_line_width + (max_line_width - min_line_width) * weight * 5\n",
        "                                  opacity = min(1.0, weight * 10)\n",
        "                                  figw.data[trace_idx].line.width = width\n",
        "                                  figw.data[trace_idx].opacity = opacity\n",
        "                                  figw.data[trace_idx].text = f\"<b>{output_labels[from_idx]} → {output_labels[to_idx]}</b><br>Weight: {weight:.6f}<br>Step {j}<br>{normalization_label}\"\n",
        "\n",
        "              figw.data[output_node_trace_idx].marker.color = [\"lightcoral\"] * num_output\n",
        "          return\n",
        "\n",
        "      if step >= num_steps or step < 0: return\n",
        "\n",
        "      with figw.batch_update():\n",
        "          figw.layout.annotations[0].text = f\"Step {step} / {num_steps-1}: {clean_label(output_tokens[step])}\"\n",
        "          generated_so_far = \" \".join([clean_label(token) for token in output_tokens[:step+1]])\n",
        "          figw.layout.annotations[2].text = f\"Generated: {generated_so_far}\"\n",
        "\n",
        "          current_attention = attention_matrices[step]\n",
        "\n",
        "          # Update Input-to-Output Connections\n",
        "          if 'input_attention' in current_attention and current_attention['input_attention'] is not None:\n",
        "              input_attn = current_attention['input_attention']\n",
        "              for conn in input_to_output_info:\n",
        "                  trace_idx, i, j = conn['trace_idx'], conn['input_idx'], conn['output_idx']\n",
        "                  if j == step and i < len(input_attn):\n",
        "                      weight = input_attn[i].item()\n",
        "                      width = min_line_width + (max_line_width - min_line_width) * weight * 5 # Scale width directly\n",
        "                      opacity = min(1.0, weight * 10) # Increase opacity scaling for joint norm\n",
        "                      is_visible = weight >= strong_threshold\n",
        "                      figw.data[trace_idx].line.width = width\n",
        "                      figw.data[trace_idx].opacity = opacity if is_visible else 0\n",
        "                      figw.data[trace_idx].text = f\"<b>{input_labels[i]} → {output_labels[j]}</b><br>Weight: {weight:.6f}<br>{normalization_label}\"\n",
        "                  else: figw.data[trace_idx].opacity = 0\n",
        "          else: # Hide all input connections\n",
        "              for conn in input_to_output_info: figw.data[conn['trace_idx']].opacity = 0\n",
        "\n",
        "          # Update Output-to-Output Connections\n",
        "          if step > 0 and 'output_attention' in current_attention and current_attention['output_attention'] is not None:\n",
        "              output_attn = current_attention['output_attention']\n",
        "              for conn in output_to_output_info:\n",
        "                  trace_idx, from_idx, to_idx = conn['trace_idx'], conn['from_idx'], conn['to_idx']\n",
        "                  if to_idx == step and from_idx < len(output_attn):\n",
        "                      weight = output_attn[from_idx].item()\n",
        "                      width = min_line_width + (max_line_width - min_line_width) * weight * 5\n",
        "                      opacity = min(1.0, weight * 10)\n",
        "                      is_visible = weight >= strong_threshold\n",
        "                      figw.data[trace_idx].line.width = width\n",
        "                      figw.data[trace_idx].opacity = opacity if is_visible else 0\n",
        "                      figw.data[trace_idx].text = f\"<b>{output_labels[from_idx]} → {output_labels[to_idx]}</b><br>Weight: {weight:.6f}<br>{normalization_label}\"\n",
        "                  else: figw.data[trace_idx].opacity = 0\n",
        "          else: # Hide all output connections\n",
        "              for conn in output_to_output_info: figw.data[conn['trace_idx']].opacity = 0\n",
        "\n",
        "          new_colors = [\"lightcoral\" if j == step else (\"lightpink\" if j < step else \"rgba(230, 230, 230, 0.8)\") for j in range(num_output)]\n",
        "          figw.data[output_node_trace_idx].marker.color = new_colors\n",
        "\n",
        "    return figw, update_visualization_joint_internal\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    DEFAULT_PROMPT = 'an old wizard was walking through the dense green forest when he saw'\n",
        "    DEFAULT_MAX_TOKENS = 20\n",
        "    DEFAULT_MODEL = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    DEFAULT_THRESHOLD_SEPARATE = 0.05\n",
        "    DEFAULT_THRESHOLD_JOINT = 0.01\n",
        "    OUTPUT_DIR = \"./attention_viz_output\"\n",
        "\n",
        "    prompt_input = widgets.Textarea(\n",
        "        value=DEFAULT_PROMPT,\n",
        "        description='Prompt:',\n",
        "        layout=widgets.Layout(width='90%', height='80px')\n",
        "    )\n",
        "\n",
        "    max_tokens_input = widgets.IntText(\n",
        "        value=DEFAULT_MAX_TOKENS,\n",
        "        description='Max Tokens:',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    threshold_separate_input = widgets.FloatText(\n",
        "        value=DEFAULT_THRESHOLD_SEPARATE,\n",
        "        description='Threshold (Separate):',\n",
        "        step=0.001, # finer control\n",
        "        layout=widgets.Layout(width='250px')\n",
        "    )\n",
        "\n",
        "    threshold_joint_input = widgets.FloatText(\n",
        "        value=DEFAULT_THRESHOLD_JOINT,\n",
        "        description='Threshold (Joint):',\n",
        "        step=0.001, # finer control\n",
        "        layout=widgets.Layout(width='250px')\n",
        "    )\n",
        "\n",
        "    run_button = widgets.Button(\n",
        "        description='Generate & Visualize Both',\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='300px')\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "    viz_area = widgets.Output()\n",
        "\n",
        "    input_widgets = widgets.VBox([\n",
        "        prompt_input,\n",
        "        widgets.HBox([max_tokens_input, threshold_separate_input, threshold_joint_input]),\n",
        "        run_button\n",
        "    ])\n",
        "    display(input_widgets, output_area, viz_area)\n",
        "\n",
        "    shared_step_dropdown = None\n",
        "    update_func_separate = None\n",
        "    update_func_joint = None\n",
        "\n",
        "    def run_visualization(b):\n",
        "        nonlocal shared_step_dropdown, update_func_separate, update_func_joint\n",
        "\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Starting visualization process...\")\n",
        "\n",
        "        with viz_area:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        prompt = prompt_input.value\n",
        "\n",
        "        # Validate max_tokens\n",
        "        try:\n",
        "            max_tokens = int(max_tokens_input.value)\n",
        "            if max_tokens < 1: max_tokens = 1; max_tokens_input.value = 1\n",
        "            if max_tokens > 100: max_tokens = 100; max_tokens_input.value = 100\n",
        "        except (ValueError, TypeError):\n",
        "            max_tokens = DEFAULT_MAX_TOKENS\n",
        "            max_tokens_input.value = max_tokens\n",
        "            with output_area: print(\"Invalid max tokens value. Using default.\")\n",
        "\n",
        "        # Validate threshold_separate\n",
        "        try:\n",
        "            threshold_separate = float(threshold_separate_input.value)\n",
        "            if not (0 <= threshold_separate <= 1.0):\n",
        "                 threshold_separate = DEFAULT_THRESHOLD_SEPARATE\n",
        "                 threshold_separate_input.value = threshold_separate\n",
        "                 with output_area: print(\"Invalid separate threshold. Clamped to [0, 1]. Using default.\")\n",
        "        except (ValueError, TypeError):\n",
        "            threshold_separate = DEFAULT_THRESHOLD_SEPARATE\n",
        "            threshold_separate_input.value = threshold_separate\n",
        "            with output_area: print(\"Invalid separate threshold value. Using default.\")\n",
        "\n",
        "        # Validate threshold_joint\n",
        "        try:\n",
        "            threshold_joint = float(threshold_joint_input.value)\n",
        "            if not (0 <= threshold_joint <= 1.0):\n",
        "                 threshold_joint = DEFAULT_THRESHOLD_JOINT\n",
        "                 threshold_joint_input.value = threshold_joint\n",
        "                 with output_area: print(\"Invalid joint threshold. Clamped to [0, 1]. Using default.\")\n",
        "        except (ValueError, TypeError):\n",
        "            threshold_joint = DEFAULT_THRESHOLD_JOINT\n",
        "            threshold_joint_input.value = threshold_joint\n",
        "            with output_area: print(\"Invalid joint threshold value. Using default.\")\n",
        "\n",
        "        if not prompt:\n",
        "            prompt = DEFAULT_PROMPT\n",
        "            prompt_input.value = prompt # Update UI\n",
        "            with output_area: print(\"Prompt is empty. Using default.\")\n",
        "\n",
        "        with output_area:\n",
        "            print(f\"Using Model: {DEFAULT_MODEL}\")\n",
        "            print(f\"Prompt: '{prompt}'\")\n",
        "            print(f\"Max Tokens: {max_tokens}\")\n",
        "            print(f\"Separate Norm Threshold: {threshold_separate:.4f}\")\n",
        "            print(f\"Joint Norm Threshold: {threshold_joint:.4f}\")\n",
        "\n",
        "        try:\n",
        "            # 1. Load Model\n",
        "            model, tokenizer, device = load_model()\n",
        "\n",
        "            # 2. Generate Text & Attentions\n",
        "            print(\"Generating text and attentions...\")\n",
        "            attentions, output_tokens, input_tokens, input_len, generated_text = generate_with_attention(\n",
        "                model, tokenizer, prompt, device, max_tokens\n",
        "            )\n",
        "\n",
        "            if attentions is None:\n",
        "                 with output_area: print(\"Generation failed or attentions not available. Cannot proceed.\")\n",
        "                 return\n",
        "\n",
        "            with output_area:\n",
        "                print(f\"\\nInput Tokens ({len(input_tokens)}): {input_tokens}\")\n",
        "                print(f\"Output Tokens ({len(output_tokens)}): {output_tokens}\")\n",
        "                print(f\"\\nGenerated text: {generated_text}\\n\")\n",
        "\n",
        "            if not output_tokens or not input_tokens:\n",
        "                with output_area: print(\"No input or output tokens were generated/found. Cannot create visualization.\")\n",
        "                return\n",
        "\n",
        "            # 3. Process Attention (Both Methods)\n",
        "            with output_area: print(\"Processing attention weights (Separate Normalization)...\")\n",
        "            attention_matrices_separate = process_attention_flow_separate(attentions, len(input_tokens), len(output_tokens))\n",
        "            with output_area: print(f\"Created {len(attention_matrices_separate)} matrices for separate norm visualization.\")\n",
        "\n",
        "            with output_area: print(\"Processing attention weights (Joint Normalization)...\")\n",
        "            attention_matrices_joint = process_attention_flow_joint(attentions, len(input_tokens), len(output_tokens))\n",
        "            with output_area: print(f\"Created {len(attention_matrices_joint)} matrices for joint norm visualization.\")\n",
        "\n",
        "            # 4. Create Visualizations\n",
        "            with output_area: print(\"Creating visualizations...\")\n",
        "            figw_separate, update_func_separate = create_flow_visualization_separate(\n",
        "                input_tokens, output_tokens, attention_matrices_separate, strong_threshold=threshold_separate\n",
        "            )\n",
        "            figw_joint, update_func_joint = create_flow_visualization_joint(\n",
        "                input_tokens, output_tokens, attention_matrices_joint, strong_threshold=threshold_joint\n",
        "            )\n",
        "\n",
        "            # 5. Create Display\n",
        "            num_steps = len(output_tokens)\n",
        "            step_options = [(f\"Step {i}: {clean_label(output_tokens[i])}\", i) for i in range(num_steps)]\n",
        "            step_options.append((\"All Connections\", -1))\n",
        "\n",
        "            shared_step_dropdown = widgets.Dropdown(\n",
        "                options=step_options,\n",
        "                value=0,\n",
        "                description='Step:',\n",
        "                layout=widgets.Layout(width='80%')\n",
        "            )\n",
        "\n",
        "            # Define the shared callback\n",
        "            def shared_dropdown_changed(change):\n",
        "                new_step = change['new']\n",
        "                if update_func_separate:\n",
        "                    update_func_separate(new_step)\n",
        "                if update_func_joint:\n",
        "                    update_func_joint(new_step)\n",
        "\n",
        "            # Link the callback\n",
        "            shared_step_dropdown.observe(shared_dropdown_changed, names='value')\n",
        "\n",
        "            if update_func_separate: update_func_separate(0)\n",
        "            if update_func_joint: update_func_joint(0)\n",
        "\n",
        "            with viz_area:\n",
        "                 display(widgets.VBox([shared_step_dropdown, figw_joint, figw_separate]))\n",
        "\n",
        "        except Exception as e:\n",
        "            with output_area:\n",
        "                print(f\"\\nAn error occurred during the process: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    run_button.on_click(run_visualization)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        main()\n",
        "    except NameError as e:\n",
        "        # Handle cases where run outside an ipython environment\n",
        "        if 'display' in str(e) or 'widgets' in str(e):\n",
        "            print(\"This script requires an environment like Jupyter Notebook/Lab or Google Colab\")\n",
        "            print(\"with ipywidgets installed to display the interactive elements.\")\n",
        "            print(\"Please run this script in such an environment.\")\n",
        "        else:\n",
        "            # Re-raise other NameErrors\n",
        "            raise e\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "y8z9J_FZwlG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}